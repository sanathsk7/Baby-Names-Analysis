---
title: 'Business Analytics & Data Science: HW2 Questions'
author: 'Sanath Shivaswamy & Christian Ghorbani'
output: html_document
---

Turn in your HW on this Rmarkdown file. Add your names here at the top. Your answers should be neatly arranged in blocks of code below; insert code blocks as needed. Use the markdown language to add any text you need to explain your homework. 

##Getting and Handling Large Data Sets

####GET THE DATA

You will acquire and analyze a real dataset on baby name popularity provided by the Social Security Administration. To warm up, we will ask you a few simple questions that can be answered by inspecting the data.

The data can be downloaded in zip format from:
http://www.ssa.gov/oact/babynames/state/namesbystate.zip  (~22MB)

####QUESTION 1
Please describe the format of the data files. Can you identify any limitations or distortions of the data.

The the data is stored in txt format and each record in a file has the format: 2-digit-state code, sex, 4 digit year of Birth, Name, Number of Occurrences.

Tie in no of occurences: When there is a tie, names are listed in alphabetical order. This sorting makes it easier to determine a name's rank. The first record for each sex & year of birth has rank 1, the second record has rank 2.

Limitation: To ensure privacy, the list of names are restricted to those with atleast 5 occurences. If a name has less than 5 occurrences for a year of birth in any state, than the sum of the state counts for that year will be less than the national count.

First, download the zip file and unzip it to grab the contents. You will see lots of different files. Store them in a directory called "namesbystate". Then we write R code to read those files and put them together. 

```{r}
state_codes = c("AK","AL","AR","AZ","CA","CO","CT","DC","DE","FL","GA","HI","IA","ID","IL","IN","KS","KY","LA","MA","MD","ME","MI","MN","MO","MS","MT","NC","ND","NE","NH","NJ","NM","NV","NY","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VA","VT","WA","WI","WV","WY");
names_data = data.frame();
```

Now we can read in this huge file in one go and convert it to a data.table.

```{r}
 for(code in state_codes){ 
  state_data = read.table(paste("namesbystate/",code,".txt",sep=""),sep=",",header=FALSE);
  names_data = rbind(names_data, state_data);
 }
library(data.table) #shows a warning message
class(names_data)
names_data_table = as.data.table(names_data)
class(names_data_table)
```

Looking at the format of the data files we see that there was a separate file for each state, and it was a csv file. The column names could have been provided. We add them in here. Then we have am organized data frame. 

```{r}
names(names_data_table) = c("State Code","Gender","Year of Birth","Name","No of Occurrences");
head(names_data_table)
```

####QUESTION 2
What is the most popular name of all time across both genders? 

```{r}
i = names_data_table[,sum(`No of Occurrences`),by=Name]
o = max(i$V1)
popular_name = as.vector(i[V1 == o,Name])
popular_name
```

####QUESTION 3
What is the most gender ambiguous name in 2013? 1945?

Most Gender Ambiguos name in 2013
```{r}
n_2013 = names_data_table[`Year of Birth` == 2013]
n_2013 = n_2013[,.(Gender,Name,`No of Occurrences`)]
m = n_2013[Gender == "M"] #Only Males
f = n_2013[Gender == "F"] #Only Females
md = m[,.(Gender,Total = sum(`No of Occurrences`)),.(Name)] 
md = md[!duplicated(md)] #removing duplicates rows
fd = f[,.(Gender,Total = sum(`No of Occurrences`)),.(Name)]
fd = fd[!duplicated(fd)]
c = merge(md,fd,by="Name") #inner join by name
e = c[,.(Name,Males = Total.x, Females = Total.y,Score = 1 - abs(as.numeric(Total.x) - as.numeric(Total.y))/(as.numeric(Total.x)+as.numeric(Total.y)))] #Calculating ambiguity score
e[Males==Females] #most gender ambiguous name in 2013
```

Now we have totals by name for the males and females. Next, we compute an ambiguity score for each name, which is defined as
$$ A = 1 - |M-F|/(M+F) $$
We see that this score is highest when the number of males (M) and the number of females (F) is equal. We add a column for this to the data.table. 
```

Now, redo for 1945.

```{r}
n_1945 = names_data_table[`Year of Birth` == 1945]
n_1945 = n_1945[,.(Gender,Name,`No of Occurrences`)]
m = n_1945[Gender == "M"] #Only Males
f = n_1945[Gender == "F"] #Only Females
md = m[,.(Gender,Total = sum(`No of Occurrences`)),.(Name)] 
md = md[!duplicated(md)] #removing duplicates rows
fd = f[,.(Gender,Total = sum(`No of Occurrences`)),.(Name)]
fd = fd[!duplicated(fd)]
c = merge(md,fd,by="Name") #inner join by name
b = c[,.(Name,Males = Total.x, Females = Total.y,Score = 1 - abs(as.numeric(Total.x) - as.numeric(Total.y))/(as.numeric(Total.x)+as.numeric(Total.y)))] #Calculating ambiguity score
b[Males==Females] #most gender ambiguous name in 1945
```


####QUESTION 4
Of the names represented in the data, find the name that has had the largest percentage increase in popularity since 1980. Largest decrease?

Largest Increase
```{r}
n_1980 = names_data_table[`Year of Birth` == 1980]
n_2014 = names_data_table[`Year of Birth` == 2014]
 
n_1980 = n_1980[,.(`Year of Birth`,Total = sum(`No of Occurrences`)),Name]
n_2014 = n_2014[,.(`Year of Birth`,Total = sum(`No of Occurrences`)),Name]

n_1980 = n_1980[!duplicated(n_1980)] #remove duplicates
n_2014 = n_2014[!duplicated(n_2014)] #remove duplicates

c = merge(n_1980,n_2014,by="Name") #inner join by name

e = c[,.(Name,`Year of Birth.x`,Total.x,`Year of Birth.y`,Total.y, PercentIncrease = ((as.numeric(Total.y)-as.numeric(Total.x))/as.numeric(Total.x))*100)] #calculating Percent increase

incr = e[PercentIncrease == max(PercentIncrease)] 
incr
```


Largest Decrease
```{r}
decr = e[PercentIncrease == min(PercentIncrease)] 
decr
```


####QUESTION 5
Can you identify names that may have had an even larger increase or decrease in popularity?

```{r}
n_1910 = names_data_table[`Year of Birth` == 1910]
n_1910 = n_1910[,.(`Year of Birth`,Total = sum(`No of Occurrences`)),Name]
n_1910 = n_1910[!duplicated(n_1910)]
b = merge(n_1910,n_2014,by="Name")
a = b[,.(Name,`Year of Birth.x`,Total.x,`Year of Birth.y`,Total.y, PercentIncrease = ((as.numeric(Total.y)-as.numeric(Total.x))/as.numeric(Total.x))*100)]

#Larger Percent Increase
a[PercentIncrease > incr$PercentIncrease]

#Larger Percent decrease
a[PercentIncrease < decr$PercentIncrease]
```

This gives interesting results, and may be used in a different way with a rolling window than using all the data. 

####QUESTION 6

What insight can you extract from this dataset? Feel free to combine the baby names data with other publicly available datasets or APIs, but be sure to include code for accessing any alternative data that you use.

```{r}
#No of baby names across states. 
 bystate = names_data_table[,.(Gender,`Year of Birth`,Name,Total = sum(`No of Occurrences`)),.(`State Code`)]
 bystate = bystate[!duplicated(bystate)]
 bystate = names_data_table[,.(Total = sum(`No of Occurrences`)),.(`State Code`)]
 
 library(ggplot2)
 mbystate = as.matrix(bystate)
 qplot(mbystate[,1],mbystate[,2],main="No of baby names across states",xlab="State Code",ylab="No of Baby Names")
#We can see from the chart that California has the most baby names and alaska has the least.
```



```{r}
#No of Unique Names by year
males = names_data_table[Gender == "M"]
females = names_data_table[Gender == "F"]
males = males[!duplicated(males)]
females = females[!duplicated(females)]
byyob = males[,.(Name),.(`Year of Birth`)]
byyob = byyob[!duplicated(byyob)]
maleunique = byyob[,.(length = length(Name)),.(`Year of Birth`)]
byyob = females[,.(Name),.(`Year of Birth`)]
byyob = byyob[!duplicated(byyob)]
femaleunique = byyob[,.(length = length(Name)),.(`Year of Birth`)]
plot(femaleunique$`Year of Birth`,femaleunique$length,type="l",main="No of Unique Names by year",xlab = "Year of Birth",ylab="No of Unique Names",col="blue")
lines(maleunique$`Year of Birth`,maleunique$length,col="red")
#Blue - No of Unique female names
#Red - No of Unique male names
#From the chart you can see that there are more unique female names than male names for each year.
```


This is an open-ended question and you are free to answer as you see fit. In fact, it would be great if you find an interesting way to look at the data that is highly interesting.

#### QUESTION 7

Go to the airlines data site: 
http://stat-computing.org/dataexpo/2009/the-data.html. 
Read in the airlines data set for 2008 into a data frame.
How many rows of data do you have?

```{r}
airline = read.csv("airline_2008/2008.csv",header = TRUE)
dim(airline) 
library(dplyr)
# rows = 7009728 columns = 29
```

#### QUESTION 8

Remove all rows of the data frame with missing data. How many rows of data do you have now?

```{r}
any(is.na(airline)) #checking to see if there any missing data
airline = na.omit(airline)
any(is.na(airline))
dim(airline)
# rows = 1524735 columns = 29
```

#### QUESTION 9

Fit one regression model each to explain "DepDelay" and "ArrDelay". Use your judgment as to which variables you might use to explain these outcomes. Use a subset of 1 million rows of the data you created with no missing data. Keep the remaining data for out-of-sample testing. 

```{r}
airline$Month = factor(airline$Month)
airline$UniqueCarrier = factor(airline$UniqueCarrier)
airline$FlightNum = factor(airline$FlightNum)
airline$TailNum = factor(airline$TailNum)
a = airline[1:1000000,]

#regression for Departure delay
x = lm(DepDelay ~ UniqueCarrier+CarrierDelay+WeatherDelay+NASDelay+SecurityDelay+LateAircraftDelay, data = a )
summary(x) 

#regression for Arrival delay
y = lm(ArrDelay ~ UniqueCarrier+DepDelay+Distance+WeatherDelay+SecurityDelay, data = a) 
summary(y) 
 
```

#### QUESTION 10

Now take the fitted regression and predict delays using the remaining data from the no-missing data set (this is the data you did not use in the fitting the model). Compare this to the actual delays and report the absolute mean error in your prediction. 

```{r}
library(hydroGOF)
sample = airline[1000001:1524735,] #remaining data

#Departure Delay Prediction
z = predict(x,sample,interval="prediction")
z = as.data.frame(z)
head(z)
z1 = as.data.frame(cbind(ActualDepartureDelay= sample$DepDelay,Prediction=z$fit))
head(z1)
mae(z1$Prediction, z1$ActualDepartureDelay)

#Arrival Delay Prediction
z = predict(y,sample,interval="prediction")
z = as.data.frame(z)
head(z)
z1 = as.data.frame(cbind(ActualArrivalDelay= sample$ArrDelay,Prediction=z$fit))
head(z1)
mae(z1$Prediction, z1$ActualArrivalDelay)

```


